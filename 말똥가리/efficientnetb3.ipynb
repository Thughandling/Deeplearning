{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import sys\n",
    "from torchinfo import summary\n",
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as T\n",
    "import torchvision.models as models\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data_utils\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import cv2\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import KFold\n",
    "from typing import Tuple, Sequence, Callable\n",
    "from torch import nn, Tensor\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained weights for efficientnet-b3\n"
     ]
    }
   ],
   "source": [
    "from efficientnet_pytorch import EfficientNet\n",
    "model = EfficientNet.from_pretrained('efficientnet-b3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MnistModel_efficientb3(nn.Module):\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "        self.backbone = EfficientNet.from_pretrained('efficientnet-b3')\n",
    "\n",
    "        self.backbone._fc = nn.Linear(1536, 512)\n",
    "        self.dropout = nn.Dropout(p=0.2)\n",
    "        self.activation = self.backbone._swish\n",
    "        self.classifier = nn.Linear(512, 26)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.backbone(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.activation(x)\n",
    "        x = self.classifier(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SAM(torch.optim.Optimizer):\n",
    "    def __init__(self, params, base_optimizer, rho=0.05, **kwargs):\n",
    "        assert rho >= 0.0, f\"Invalid rho, should be non-negative: {rho}\"\n",
    "\n",
    "        defaults = dict(rho=rho, **kwargs)\n",
    "        super(SAM, self).__init__(params, defaults)\n",
    "\n",
    "        self.base_optimizer = base_optimizer(self.param_groups, **kwargs)\n",
    "        self.param_groups = self.base_optimizer.param_groups\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def first_step(self, zero_grad=False):\n",
    "        grad_norm = self._grad_norm()\n",
    "        for group in self.param_groups:\n",
    "            scale = group[\"rho\"] / (grad_norm + 1e-12)\n",
    "\n",
    "            for p in group[\"params\"]:\n",
    "                if p.grad is None: continue\n",
    "                e_w = p.grad * scale.to(p)\n",
    "                p.add_(e_w)  # climb to the local maximum \"w + e(w)\"\n",
    "                self.state[p][\"e_w\"] = e_w\n",
    "\n",
    "        if zero_grad: self.zero_grad()\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def second_step(self, zero_grad=False):\n",
    "        for group in self.param_groups:\n",
    "            for p in group[\"params\"]:\n",
    "                if p.grad is None: continue\n",
    "                p.sub_(self.state[p][\"e_w\"])  # get back to \"w\" from \"w + e(w)\"\n",
    "\n",
    "        self.base_optimizer.step()  # do the actual \"sharpness-aware\" update\n",
    "\n",
    "        if zero_grad: self.zero_grad()\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def step(self, closure=None):\n",
    "        assert closure is not None, \"Sharpness Aware Minimization requires closure, but it was not provided\"\n",
    "        closure = torch.enable_grad()(closure)  # the closure should do a full forward-backward pass\n",
    "\n",
    "        self.first_step(zero_grad=True)\n",
    "        closure()\n",
    "        self.second_step()\n",
    "\n",
    "    def _grad_norm(self):\n",
    "        shared_device = self.param_groups[0][\"params\"][0].device  # put everything on the same device, in case of model parallelism\n",
    "        norm = torch.norm(\n",
    "                torch.stack([\n",
    "                  p.grad.norm(p=2).to(shared_device)\n",
    "                    for group in self.param_groups for p in group[\"params\"]\n",
    "                      if p.grad is not None\n",
    "                    ]),\n",
    "                    p=2\n",
    "               )\n",
    "        return norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "data_dir =  r\"C:\\dacon\\\\data\\\\dirty_mnist_2nd\\\\\"\n",
    "train_label_dir = pd.read_csv(r\"C:\\dacon\\\\data\\\\dirty_mnist_2nd_answer.csv\")\n",
    "\n",
    "    \n",
    "kfold = KFold(n_splits=5, shuffle=False, random_state=0)\n",
    "epoch_size = 5\n",
    "batch_size = 8\n",
    "    \n",
    "best_model = []\n",
    "for fold_index, (train_idx, valid_idx) in enumerate(kfold.split(train_label_dir), 1):\n",
    "        \n",
    "        print(f'[fold:] {fold_index}')\n",
    "        torch.cuda.empty_cache()#gpu에서 메모리 내려놓음\n",
    "        \n",
    "        train_label = train_label_dir.iloc[train_idx]\n",
    "        valid_label = train_label_dir.iloc[valid_idx]\n",
    "        train_image = Generator(r\"C:\\dacon\\\\data\\\\dirty_mnist_2nd\\\\\", train_label)\n",
    "        valid_image = Generator(r\"C:\\dacon\\\\data\\\\dirty_mnist_2nd\\\\\", valid_label)\n",
    "        \n",
    "        train_loader = DataLoader(\n",
    "            train_image,\n",
    "            batch_size=batch_size,\n",
    "            shuffle = False,\n",
    "            num_workers=0                  \n",
    "        )\n",
    "        valid_loader = DataLoader(\n",
    "            valid_image,\n",
    "            batch_size=batch_size,\n",
    "            shuffle=False,\n",
    "            num_workers=0                  \n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained weights for efficientnet-b3\n",
      "=========================================================================================================\n",
      "Layer (type:depth-idx)                                  Output Shape              Param #\n",
      "=========================================================================================================\n",
      "├─EfficientNet: 1-1                                     [1, 512]                  --\n",
      "|    └─Conv2dStaticSamePadding: 2-1                     [1, 40, 128, 128]         --\n",
      "|    |    └─ZeroPad2d: 3-1                              [1, 3, 258, 258]          --\n",
      "|    └─BatchNorm2d: 2-2                                 [1, 40, 128, 128]         80\n",
      "|    └─MemoryEfficientSwish: 2-3                        [1, 40, 128, 128]         --\n",
      "├─MemoryEfficientSwish: 1-2                             [1, 40, 128, 128]         --\n",
      "├─EfficientNet: 1                                       []                        --\n",
      "|    └─ModuleList: 2                                    []                        --\n",
      "|    |    └─MBConvBlock: 3-2                            [1, 24, 128, 128]         2,298\n",
      "|    |    └─MBConvBlock: 3-3                            [1, 24, 128, 128]         1,206\n",
      "|    |    └─MBConvBlock: 3-4                            [1, 32, 64, 64]           11,878\n",
      "|    |    └─MBConvBlock: 3-5                            [1, 32, 64, 64]           18,120\n",
      "|    |    └─MBConvBlock: 3-6                            [1, 32, 64, 64]           18,120\n",
      "|    |    └─MBConvBlock: 3-7                            [1, 48, 32, 32]           24,296\n",
      "|    |    └─MBConvBlock: 3-8                            [1, 48, 32, 32]           43,308\n",
      "|    |    └─MBConvBlock: 3-9                            [1, 48, 32, 32]           43,308\n",
      "|    |    └─MBConvBlock: 3-10                           [1, 96, 16, 16]           52,620\n",
      "|    |    └─MBConvBlock: 3-11                           [1, 96, 16, 16]           146,520\n",
      "|    |    └─MBConvBlock: 3-12                           [1, 96, 16, 16]           146,520\n",
      "|    |    └─MBConvBlock: 3-13                           [1, 96, 16, 16]           146,520\n",
      "|    |    └─MBConvBlock: 3-14                           [1, 96, 16, 16]           146,520\n",
      "|    |    └─MBConvBlock: 3-15                           [1, 136, 16, 16]          178,856\n",
      "|    |    └─MBConvBlock: 3-16                           [1, 136, 16, 16]          302,226\n",
      "|    |    └─MBConvBlock: 3-17                           [1, 136, 16, 16]          302,226\n",
      "|    |    └─MBConvBlock: 3-18                           [1, 136, 16, 16]          302,226\n",
      "|    |    └─MBConvBlock: 3-19                           [1, 136, 16, 16]          302,226\n",
      "|    |    └─MBConvBlock: 3-20                           [1, 232, 8, 8]            380,754\n",
      "|    |    └─MBConvBlock: 3-21                           [1, 232, 8, 8]            849,642\n",
      "|    |    └─MBConvBlock: 3-22                           [1, 232, 8, 8]            849,642\n",
      "|    |    └─MBConvBlock: 3-23                           [1, 232, 8, 8]            849,642\n",
      "|    |    └─MBConvBlock: 3-24                           [1, 232, 8, 8]            849,642\n",
      "|    |    └─MBConvBlock: 3-25                           [1, 232, 8, 8]            849,642\n",
      "|    |    └─MBConvBlock: 3-26                           [1, 384, 8, 8]            1,039,258\n",
      "|    |    └─MBConvBlock: 3-27                           [1, 384, 8, 8]            2,244,960\n",
      "|    └─Conv2dStaticSamePadding: 2-4                     [1, 1536, 8, 8]           --\n",
      "|    |    └─Identity: 3-28                              [1, 384, 8, 8]            --\n",
      "|    └─BatchNorm2d: 2-5                                 [1, 1536, 8, 8]           3,072\n",
      "|    └─MemoryEfficientSwish: 2-6                        [1, 1536, 8, 8]           --\n",
      "├─MemoryEfficientSwish: 1-3                             [1, 1536, 8, 8]           --\n",
      "├─EfficientNet: 1                                       []                        --\n",
      "|    └─AdaptiveAvgPool2d: 2-7                           [1, 1536, 1, 1]           --\n",
      "|    └─Dropout: 2-8                                     [1, 1536]                 --\n",
      "|    └─Linear: 2-9                                      [1, 512]                  786,944\n",
      "├─Dropout: 1-4                                          [1, 512]                  --\n",
      "├─EfficientNet: 1                                       []                        --\n",
      "|    └─MemoryEfficientSwish: 2-10                       [1, 512]                  --\n",
      "├─MemoryEfficientSwish: 1-5                             [1, 512]                  --\n",
      "├─Linear: 1-6                                           [1, 26]                   13,338\n",
      "=========================================================================================================\n",
      "Total params: 10,905,610\n",
      "Trainable params: 10,905,610\n",
      "Non-trainable params: 0\n",
      "Total mult-adds (G): 9.68\n",
      "=========================================================================================================\n",
      "Input size (MB): 0.79\n",
      "Forward/backward pass size (MB): 135.70\n",
      "Params size (MB): 43.62\n",
      "Estimated Total Size (MB): 180.11\n",
      "=========================================================================================================\n"
     ]
    }
   ],
   "source": [
    "def seed_everything(seed: int = 42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)  # type: ignore\n",
    "    torch.backends.cudnn.deterministic = True  # type: ignore\n",
    "    torch.backends.cudnn.benchmark = True  # type: ignore\n",
    "seed_everything(42)\n",
    "\n",
    "class MnistDataset(Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        dir: os.PathLike,\n",
    "        image_ids: os.PathLike,\n",
    "        transforms: Sequence[Callable]\n",
    "    ) -> None:\n",
    "        self.dir = dir\n",
    "        self.transforms = transforms\n",
    "\n",
    "        self.labels = {}\n",
    "        with open(image_ids, 'r') as f:\n",
    "            reader = csv.reader(f)\n",
    "            next(reader)\n",
    "            for row in reader:\n",
    "                self.labels[int(row[0])] = list(map(int, row[1:]))\n",
    "\n",
    "        self.image_ids = list(self.labels.keys())\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.image_ids)\n",
    "\n",
    "    def __getitem__(self, index: int) -> Tuple[Tensor]:\n",
    "        image_id = self.image_ids[index]\n",
    "        image = Image.open(\n",
    "            os.path.join(\n",
    "                self.dir, f'{str(image_id).zfill(5)}.png')).convert('RGB')\n",
    "        target = np.array(self.labels.get(image_id)).astype(np.float32)\n",
    "\n",
    "        if self.transforms is not None:\n",
    "            image = self.transforms(image)\n",
    "\n",
    "        return image, target\n",
    "\n",
    "transforms_train = T.Compose([\n",
    "    #transforms.RandomHorizontalFlip(p=0.5),\n",
    "    #transforms.RandomVerticalFlip(p=0.5),\n",
    "    T.RandomRotation(180, expand=False),\n",
    "    T.ToTensor(),\n",
    "    T.Normalize(\n",
    "        [0.485, 0.456, 0.406],\n",
    "        [0.229, 0.224, 0.225]\n",
    "    )\n",
    "])\n",
    "\n",
    "trainset = MnistDataset(r\"C:\\dacon\\\\data\\\\dirty_mnist_2nd\", r\"C:\\dacon\\\\data\\\\dirty_mnist_2nd_answer.csv\", transforms_train)\n",
    "\n",
    "dataset_size = len(trainset)\n",
    "validation_split = 0.1\n",
    "indices = list(range(dataset_size))\n",
    "split = int(np.floor(validation_split * dataset_size))\n",
    "\n",
    "np.random.shuffle(indices)\n",
    "train_indices, val_indices = indices[split:], indices[:split]\n",
    "\n",
    "# Creating PT data samplers and loaders:\n",
    "train_sampler = SubsetRandomSampler(train_indices)\n",
    "valid_sampler = SubsetRandomSampler(val_indices)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(trainset, batch_size=8, sampler=train_sampler, num_workers=8)\n",
    "validation_loader = torch.utils.data.DataLoader(trainset, batch_size=8, sampler=valid_sampler, num_workers=4)\n",
    "\n",
    "\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = MnistModel_efficientb3().to(device)\n",
    "print(summary(model, input_size=(1, 3, 256, 256), verbose=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.AdamW(model.parameters(), lr=1e-3)\n",
    "#optimizer = SAM(model.parameters(), base_optimizer, lr=1e-3)\n",
    "\n",
    "criterion = nn.MultiLabelSoftMarginLoss()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 60\n",
    "\n",
    "best_loss = 1e10\n",
    "best_acc = 0\n",
    "no_improvement = 0\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (images, targets) in enumerate(train_loader):\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        images = images.to(device) \n",
    "        targets = targets.to(device)\n",
    "        \n",
    "        images, targets_a, targets_b, lam = mixup_data(images, targets)\n",
    "        images, targets_a, targets_b = map(Variable, (images, targets_a, targets_b))\n",
    "\n",
    "        outputs = model(images)\n",
    "        loss = mixup_criterion(criterion, outputs, targets_a, targets_b, lam)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        #mixup_criterion(criterion, model(images), targets_a, targets_b, lam).backward()\n",
    "        #optimizer.second_step(zero_grad=True)\n",
    "\n",
    "        outputs = outputs > 0.5\n",
    "        acc = (outputs == targets).float().mean()\n",
    "        print(f'EPOCH: {epoch}/{num_epochs} | {i} / {len(train_loader)} | LOSS: {loss.item():.5f}, ACCURACY: {acc.item():.5f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
